{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148b2f7-42c9-4c8a-826f-54b2c7520fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Non_Demented', 'Signs_of_Dementia']\n"
     ]
    }
   ],
   "source": [
    "#First Model: Non Dementia vs Signs of Dementia\n",
    "#getting the data for the first model ready using DataLoader \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  \n",
    "])\n",
    "dataset = datasets.ImageFolder(root=\"Processed_MRI_Data_For_First_Model\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(dataset.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868848dc-e61c-461a-8def-9b45d957a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holde\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\holde\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Bring in pretrained resnet50\n",
    "import torchvision.models as models\n",
    "res_net_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303739d-2b25-42bf-af35-418333b56d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 label count: Counter({0: 29, 1: 21})\n",
      "Batch 2 label count: Counter({1: 27, 0: 23})\n",
      "Batch 3 label count: Counter({0: 28, 1: 22})\n",
      "Batch 4 label count: Counter({0: 31, 1: 19})\n",
      "Batch 5 label count: Counter({1: 27, 0: 23})\n",
      "Batch 6 label count: Counter({0: 27, 1: 23})\n",
      "Batch 7 label count: Counter({0: 28, 1: 22})\n",
      "Batch 8 label count: Counter({0: 29, 1: 21})\n",
      "Batch 9 label count: Counter({0: 29, 1: 21})\n",
      "Batch 10 label count: Counter({1: 26, 0: 24})\n",
      "Batch 11 label count: Counter({1: 28, 0: 22})\n",
      "Batch 12 label count: Counter({0: 27, 1: 23})\n",
      "Batch 13 label count: Counter({1: 29, 0: 21})\n",
      "Batch 14 label count: Counter({1: 29, 0: 21})\n",
      "Batch 15 label count: Counter({1: 25, 0: 25})\n",
      "-----------------------------\n",
      "Batch 1 label count: Counter({0: 40, 1: 10})\n",
      "Batch 2 label count: Counter({0: 40, 1: 10})\n",
      "Batch 3 label count: Counter({0: 41, 1: 9})\n",
      "Batch 4 label count: Counter({0: 40, 1: 10})\n",
      "Batch 5 label count: Counter({0: 42, 1: 8})\n",
      "Batch 6 label count: Counter({0: 34, 1: 16})\n",
      "Batch 7 label count: Counter({0: 41, 1: 9})\n",
      "Batch 8 label count: Counter({0: 36, 1: 14})\n",
      "Batch 9 label count: Counter({0: 39, 1: 11})\n",
      "Batch 10 label count: Counter({0: 39, 1: 11})\n",
      "Batch 11 label count: Counter({0: 38, 1: 12})\n",
      "Batch 12 label count: Counter({0: 43, 1: 7})\n",
      "Batch 13 label count: Counter({0: 33, 1: 17})\n",
      "Batch 14 label count: Counter({0: 36, 1: 14})\n",
      "Batch 15 label count: Counter({0: 40, 1: 10})\n"
     ]
    }
   ],
   "source": [
    "#create the training set and validation set, but for each batch, weight the images so that\n",
    "#we get close to an equal number of images of each class in each batch.\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "train_indices, val_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.targets)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "#get the count of each class\n",
    "class_counts = [0,0]\n",
    "for class_i in train_indices:\n",
    "    c_i = dataset.targets[class_i]\n",
    "    class_counts[c_i] += 1\n",
    "class_proportions = [(1/class_counts[0]) , (1/class_counts[1])]\n",
    "class_proportions = [prop * 100000 for prop in class_proportions]\n",
    "#class_proportions = [class_proportions[0] * 100000 , class_proportions[1]* 1000000]\n",
    "\n",
    "sample_weights = []\n",
    "for idx in train_indices:  # Iterate through the training indices\n",
    "    label = dataset.targets[idx]\n",
    "    sample_weights.append(class_proportions[0] if label == 0 else class_proportions[1])\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, sampler=sampler)\n",
    "#print(sample_weights)\n",
    "eval_loader = DataLoader(val_dataset, batch_size = 50, shuffle = False)\n",
    "\n",
    "num_batches_to_check = 15  # Number of batches you want to inspect\n",
    "\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i >= num_batches_to_check:\n",
    "        break\n",
    "    label_count = Counter(labels.tolist())  # Count the labels in the batch\n",
    "    print(f\"Batch {i + 1} label count:\", label_count)\n",
    "print(\"-----------------------------\")\n",
    "for i, (images, labels) in enumerate(eval_loader):\n",
    "    if i >= num_batches_to_check:\n",
    "        break\n",
    "    label_count = Counter(labels.tolist())  # Count the labels in the batch\n",
    "    print(f\"Batch {i + 1} label count:\", label_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a80da1-3910-4875-b53d-499605d5a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the final layer \n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "res_net_model.fc = torch.nn.Linear(res_net_model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73b7cf-101f-419d-b876-78b975ae3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function and the optimizer\n",
    "#started with learning rate of 0.01, but as I got better results I manually dropped it and observed the loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(res_net_model.parameters(), lr=0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9ad025-961d-47bb-a90b-3576af62e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#begin forward and backwards propogation\\nfrom collections import Counter\\nnum_epochs = 1\\nfor epoch in range(num_epochs):\\n    res_net_model.train()\\n    running_loss = 0.0\\n    #iterate through a batch\\n    print(len(train_loader))\\n    for ind, (inputs, labels) in enumerate(train_loader):\\n        #print(ind)\\n        optimizer.zero_grad() #zero out gradients\\n        batch_outputs = res_net_model(inputs)\\n        loss = criterion(batch_outputs, labels)\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update model weights\\n\\n        running_loss += loss.item()\\n        if (ind + 1) % 10 == 0:\\n            label_count = Counter(labels.tolist())  # Count the labels in the batch\\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{ind+1}/{len(train_loader)}], Loss: {loss.item():.4f} label count: {label_count}\")\\n        if(ind + 1) % 150 == 0:\\n            torch.save(res_net_model.state_dict(), f\"checkpoint_step_{ind+1}.pth\")\\n            print(\"Model weights saved\")            \\n\\n    #start evaluation\\n    res_net_model.eval()\\n    correct, total = 0 , 0\\n    with torch.no_grad():\\n        for inputs, labels in eval_loader:\\n            eval_outputs = res_net_model(inputs)\\n            _, predicted = torch.max(eval_outputs, 1)  # Get predicted class\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n    accuracy = 100 * correct / total\\n    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\\n            \\ntorch.save(res_net_model.state_dict(), \"progress_model_weights.pth\")\\n\\n        \\n    \\n    \\n    \\n        \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#begin forward and backwards propogation\n",
    "from collections import Counter\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    res_net_model.train()\n",
    "    running_loss = 0.0\n",
    "    #iterate through a batch\n",
    "    print(len(train_loader))\n",
    "    for ind, (inputs, labels) in enumerate(train_loader):\n",
    "        #print(ind)\n",
    "        optimizer.zero_grad() #zero out gradients\n",
    "        batch_outputs = res_net_model(inputs)\n",
    "        loss = criterion(batch_outputs, labels)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (ind + 1) % 10 == 0:\n",
    "            label_count = Counter(labels.tolist())  # Count the labels in the batch\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{ind+1}/{len(train_loader)}], Loss: {loss.item():.4f} label count: {label_count}\")\n",
    "        if(ind + 1) % 150 == 0:\n",
    "            torch.save(res_net_model.state_dict(), f\"checkpoint_step_{ind+1}.pth\")\n",
    "            print(\"Model weights saved\")            \n",
    "\n",
    "    #start evaluation\n",
    "    res_net_model.eval()\n",
    "    correct, total = 0 , 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in eval_loader:\n",
    "            eval_outputs = res_net_model(inputs)\n",
    "            _, predicted = torch.max(eval_outputs, 1)  # Get predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "            \n",
    "torch.save(res_net_model.state_dict(), \"progress_model_weights.pth\")\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "'''       \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef81f74-2ab1-4725-a80b-147f3b2dfad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nres_net_model.load_state_dict(torch.load(\"checkpoint_step_900.pth\"))\\nres_net_model.eval()\\ncorrect, total = 0 , 0\\nwith torch.no_grad():\\n    for ind, (inputs, labels) in enumerate(eval_loader):\\n        eval_outputs = res_net_model(inputs)\\n        _, predicted = torch.max(eval_outputs, 1)  # Get predicted class\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n        print(f\"Batch {ind+1}: Accuracy: {correct / total} , {correct}: {total}\")\\n\\naccuracy = 100 * correct / total\\nprint(f\"Evaluation Accuracy: {accuracy:.2f}%\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "res_net_model.load_state_dict(torch.load(\"checkpoint_step_900.pth\"))\n",
    "res_net_model.eval()\n",
    "correct, total = 0 , 0\n",
    "with torch.no_grad():\n",
    "    for ind, (inputs, labels) in enumerate(eval_loader):\n",
    "        eval_outputs = res_net_model(inputs)\n",
    "        _, predicted = torch.max(eval_outputs, 1)  # Get predicted class\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(f\"Batch {ind+1}: Accuracy: {correct / total} , {correct}: {total}\")\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1060c3-4ad4-4e9c-9e03-63dcdd74c6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nres_net_model.load_state_dict(torch.load(\"NEWLY_SAVED_MODEL_1_Part_2_Updated_checkpoint_step_60.pth\"))\\nfrom collections import Counter\\nnum_epochs = 1\\nfor epoch in range(num_epochs):\\n    res_net_model.train()\\n    running_loss = 0.0\\n    #iterate through a batch\\n    print(len(train_loader))\\n    for ind, (inputs, labels) in enumerate(train_loader):\\n        optimizer.zero_grad() #zero out gradients\\n        batch_outputs = res_net_model(inputs)\\n        loss = criterion(batch_outputs, labels)\\n        loss.backward()  # Backpropagation\\n        optimizer.step()  # Update model weights\\n\\n        running_loss += loss.item()\\n        label_count = Counter(labels.tolist())  # Count the labels in the batch\\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{ind+1}/{len(train_loader)}], Loss: {loss.item():.4f} label count: {label_count}\")\\n        if (ind + 1) % 20 == 0:\\n            save = input(\"Save this model?\")\\n            if save == \"y\":\\n                torch.save(res_net_model.state_dict(), f\"NEWLY_SAVED_MODEL_1_Part_2_Updated_checkpoint_step_{ind+1}.pth\")\\n                print(\"Model weights saved\")            \\n                   \\n\\n    accuracy = 100 * correct / total\\n    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\\n            \\n#torch.save(res_net_model.state_dict(), \"progress_model_weights.pth\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training algorithm I used while observing. Load in the weights previously gotten during the training. \n",
    "#train it, observe the loss each batch, every two batches, you can be asked to save the current weights.\n",
    "#This allows us to view the loss closely, and I believe this allows us to reach the global minimum faster.\n",
    "res_net_model.load_state_dict(torch.load(\"NEWLY_SAVED_MODEL_1_Part_2_Updated_checkpoint_step_60.pth\"))\n",
    "from collections import Counter\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    res_net_model.train()\n",
    "    running_loss = 0.0\n",
    "    #iterate through a batch\n",
    "    print(len(train_loader))\n",
    "    for ind, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() #zero out gradients\n",
    "        batch_outputs = res_net_model(inputs)\n",
    "        loss = criterion(batch_outputs, labels)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        label_count = Counter(labels.tolist())  # Count the labels in the batch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{ind+1}/{len(train_loader)}], Loss: {loss.item():.4f} label count: {label_count}\")\n",
    "        if (ind + 1) % 20 == 0:\n",
    "            save = input(\"Save this model?\")\n",
    "            if save == \"y\":\n",
    "                torch.save(res_net_model.state_dict(), f\"NEWLY_SAVED_MODEL_1_Part_2_Updated_checkpoint_step_{ind+1}.pth\")\n",
    "                print(\"Model weights saved\")            \n",
    "                   \n",
    "\n",
    "    \n",
    "            \n",
    "#torch.save(res_net_model.state_dict(), \"progress_model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33ae00-31ca-4540-a777-4e7476032a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the weights\n",
    "res_net_model.load_state_dict(torch.load(\"NEWLY_SAVED_MODEL_1_Part_2_Updated_checkpoint_step_500.pth\"))\n",
    "res_net_model.eval()\n",
    "correct, total = 0 , 0\n",
    "with torch.no_grad():\n",
    "    for ind, (inputs, labels) in enumerate(eval_loader):\n",
    "        eval_outputs = res_net_model(inputs)\n",
    "        _, predicted = torch.max(eval_outputs, 1)  # Get predicted class\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(f\"Batch {ind+1}: Accuracy: {correct / total} , {correct}: {total}\")\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d01442-e115-4e8e-8586-6d51e2efb7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
